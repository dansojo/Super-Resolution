{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5eCUE7KzYa0IrQWy7aH0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dansojo/Super-Resolution/blob/main/ESPCN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xta82yODJolM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.image as img\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, Lambda\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(lr_directory, hr_directory, target_size=(512, 512)):\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    # 파일 목록을 안전하게 가져오기\n",
        "    try:\n",
        "        lr_filenames = sorted(os.listdir(lr_directory))\n",
        "        hr_filenames = sorted(os.listdir(hr_directory))\n",
        "    except FileNotFoundError:\n",
        "        print(\"Directory not found\")\n",
        "        return [], []\n",
        "\n",
        "    for lr_file, hr_file in zip(lr_filenames, hr_filenames):\n",
        "        try:\n",
        "            lr_image_path = os.path.join(lr_directory, lr_file)\n",
        "            hr_image_path = os.path.join(hr_directory, hr_file)\n",
        "\n",
        "            lr_img = Image.open(lr_image_path).resize(target_size, Image.BICUBIC)\n",
        "            hr_img = Image.open(hr_image_path).resize((2048, 2048), Image.BICUBIC)\n",
        "\n",
        "            x_train.append(np.array(lr_img))\n",
        "            y_train.append(np.array(hr_img))\n",
        "        except (IOError, OSError):\n",
        "            print(f\"Error processing {lr_file} or {hr_file}\")\n",
        "\n",
        "    if len(x_train) > 0 and len(y_train) > 0:\n",
        "        x_train = np.array(x_train) / 255.0\n",
        "        y_train = np.array(y_train) / 255.0\n",
        "\n",
        "    return x_train, y_train"
      ],
      "metadata": {
        "id": "EXR3ojN-KvRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def ESPCN(scale_factor, num_channels=3):\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(None, None, num_channels), name='input_layer')\n",
        "\n",
        "    # Feature extraction layer\n",
        "    features = Conv2D(filters=64, kernel_size=5, padding='same', activation='tanh', name='feature_extraction')(input_layer)\n",
        "\n",
        "    # Non-linear mapping layer\n",
        "    mapping = Conv2D(filters=32, kernel_size=3, padding='same', activation='tanh', name='non_linear_mapping')(features)\n",
        "\n",
        "    # Sub-pixel convolution layer for upscaling\n",
        "    subpixel = Conv2D(filters=num_channels * (scale_factor ** 2), kernel_size=3, padding='same', activation='tanh', name='subpixel_convolution')(mapping)\n",
        "\n",
        "    # Rearrange the data in the subpixel layer to form the final high resolution image\n",
        "    output_layer = Lambda(lambda x: tf.nn.depth_to_space(x, scale_factor), name='pixel_shuffle')(subpixel)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name='ESPCN')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "IjhTgKbyMau3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QNIKi5VWTpcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 데이터셋 로드 경로\n",
        "dataset_hr_directory = '/content/drive/MyDrive/Super-Resolution/SR_data_set/train/hr'\n",
        "dataset_lr_directory = '/content/drive/MyDrive/Super-Resolution/SR_data_set/train/lr'\n",
        "x_train, y_train = load_images(dataset_lr_directory, dataset_hr_directory)"
      ],
      "metadata": {
        "id": "jTHlqg8WMiid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the espcn_model\n",
        "scale_factor = 4  # Example scale factor\n",
        "espcn_model = ESPCN(scale_factor=scale_factor, num_channels=3)\n",
        "espcn_model.summary()\n",
        "\n",
        "# Compile the model\n",
        "espcn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "espcn_model.fit(x_train, y_train, batch_size=16, epochs=100)\n",
        "\n",
        "espcn_model.save('espcn_model_v1.h5')"
      ],
      "metadata": {
        "id": "GoMY3KdeMdCF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}